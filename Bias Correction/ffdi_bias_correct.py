import numpy as np
import xarray as xr
from qme_utils import *
from qme_vars import *


def apply_mean_values(data, mean_values, start_year = 0):
    """
    Applies the generated mean values to the corresponding years of the given data set.
    Inputs:
    data - the data to apply the mean values to
    mean_values - the mean values generated by qme_train.find_means. Pass the argument with a leading minus sign to subtract the values instead
    start_year - the year (relative to the domain, with 0 being the first) at which to start applying the mean values. Every year before is unchanged
    """
    year_values = data.time.values.astype('datetime64[Y]').astype(int)
    min_year = year_values.min()
    year_values = year_values - min_year
    
    def apply_temp(data_loc, mean_loc):
        adjusted = data_loc.copy()
        for i in range(len(data_loc)):
            year = int(year_values[i])
            if year >= start_year:  
                adjusted[i] += mean_loc[year]
        return adjusted
        
    return xr.apply_ufunc(apply_temp, data, mean_values, input_core_dims = [["time"], ["values"]], 
                          output_core_dims = [["time"]], vectorize = True, keep_attrs = True,
                          output_dtypes = [np.float32], dask = 'parallelized')


def apply_bc(var, mdl, bc):
    """
    Applies the bias correction factors to the model data.
    Inputs:
    mdl - the model data
    bc - the bias correction factors
    var - the variable being corrected
    """
    var = get_qme_var(var)
    reso = var.bin_count()
    month_values = mdl.time.values.astype('datetime64[M]').astype(int) % 12
    def apply(mdl_loc, bc_loc):
        adjusted = var.scale_data(var.limit_data(mdl_loc))

        # special rounding function used to correct Numpy rounding towards evens - see comments in qme_utils
        rounded = round_half_up(adjusted)

        # original version
        # rounded = np.round(adjusted).astype(int)
        
        for i, value in enumerate(rounded):
            # check for out of bounds in case of funky numbers when dealing with NaNs
            if value >= 0 and value < reso:
                adjusted[i] += bc_loc[month_values[i]][value]
        adjusted = var.unscale_data(adjusted)
        return adjusted
        
    return xr.apply_ufunc(apply, mdl, bc, input_core_dims = [["time"], ["month", "values"]], 
                          output_core_dims = [["time"]], vectorize = True, keep_attrs = True, 
                          output_dtypes = [np.float32], dask = 'parallelized')

def standardise_latlon(ds, digits=2):
    """
    This function rounds the latitude / longitude coordinates to the 4th digit, because some datasets
    seem to have strange digits (e.g., 50.00000001 instead of 50.0), which prevents merging of data.
    """
    ds = ds.assign_coords({"lat": np.round(ds.lat, digits).astype('float64')})
    ds = ds.assign_coords({"lon": np.round(ds.lon, digits).astype('float64')})
    return ds

def preprocess_ds(ds):
    # Set errant time values to datetime (no-leap models will have conflicting time values otherwise)
    if ds.time.dtype == "O":
        ds = ds.assign_coords(time=ds.indexes['time'].to_datetimeindex())

    ds = standardise_latlon(ds)
    ds = ds.drop_vars([item for item in ('height', 'lat_bnds', 'lon_bnds', 'time_bnds') if item in ds.variables or item in ds.dims])

    # Round all time values to midday for consistency
    ds = ds.assign_coords(time=ds.time.dt.floor("D") + np.timedelta64(12, 'h'))

    return ds

def qme_run(var, ref_data, hist_data, fut_data=None):
    # Debugging print statement
    print(f"Running QME for variable: {var}")

    # Parameters for training
    params = {
        "xtr": 3,
        "cal_smth": 21,
        "mthd": '_quick',
        "mn_smth": '_3mn' if var == "pr" else '',
        "ssze_lim": 50,
        "mltp": False,
        "lmt": 1.5 if var == "pr" else -1,
        "lmt_thresh": 10,
        "force_zero": var == "pr"
    }

    # Whether to account for trend with moving average
    account_trend = var in ("tasmax", "tasmin")

    # Create distributions histograms
    print(f"Creating distribution histograms for {var}")
    dist_obs = make_dist(var, ref_data).chunk({"values": -1, "month": -1})
    dist_mdl = make_dist(var, hist_data).chunk({"values": -1, "month": -1})

    # Apply QME method to distributions to calculate bias correction factors
    print(f"Calculating bias correction factors for {var}")
    dist_bc = calc_qme(var, dist_mdl, dist_obs, **params).chunk({"values": -1, "month": -1}).persist()

    mdl_mean = None
    fut_mdl_mean = None

    # Apply bias correction to model data
    if account_trend:
        print(f"Applying mean values for trend account for {var}")
        # Find model means
        mdl_mean = find_means(hist_data).chunk({"values": -1}).persist()
        mdl_bc = apply_mean_values(hist_data, -mdl_mean, 15)
        mdl_bc = apply_bc(var, mdl_bc, dist_bc)
        mdl_bc = apply_mean_values(mdl_bc, mdl_mean, 15)
    else:
        # Apply the bias correction to model data without means
        mdl_bc = apply_bc(var, hist_data, dist_bc)

    if fut_data:
        if account_trend:
            print(f"Applying future mean values for trend account for {var}")
            # Find model means (note that this is calculated slightly differently for future data)
            fut_mdl_mean = future_means(hist_data, fut_data).chunk({"values": -1}).persist()
            fut_bc = apply_mean_values(fut_data, -fut_mdl_mean)
            fut_bc = apply_bc(var, fut_bc, dist_bc)
            fut_bc = apply_mean_values(fut_bc, fut_mdl_mean)
        else:
            # Apply the bias correction to future data without means
            fut_bc = apply_bc(var, fut_data, dist_bc)

    # Write bias-corrected historical data
    print(f"Writing bias-corrected historical data for {var}")
    mdl_bc = mdl_bc.chunk({'time': 'auto', 'lat': -1, 'lon': -1})
    mdl_bc.to_netcdf(outdir + f'{var}_historical.nc',  # Short output name for the sake of example
                     unlimited_dims=['time'],
                     encoding={'time': {'dtype': 'float32'}})

    if fut_data:
        # Write bias-corrected projection data
        print(f"Writing bias-corrected projection data for {var}")
        fut_bc = fut_bc.chunk({'time': 'auto', 'lat': -1, 'lon': -1})
        fut_bc.to_netcdf(outdir + f'{var}_projection.nc',
                         unlimited_dims=['time'],
                         encoding={'time': {'dtype': 'float32'}})

    # Clean up for next round (just these in particular as they were called with "persist" to keep them in memory for the various operations they were required for)
    del dist_bc
    if mdl_mean is not None:
        del mdl_mean
    if fut_mdl_mean is not None:
        del fut_mdl_mean

    print(f"QME run completed for {var}")

if __name__ == "__main__":
    # Set up Dask cluster from the scheduler
    client = Client()

    # Upload the four files containing the QME functions so that the workers can access them
    # client.upload_file("qme_utils.py")
    # client.upload_file("qme_vars.py")
    # client.upload_file("qme_train.py")
    # client.upload_file("qme_apply.py")

    var = "FFDI"

    chunk_size = 25

    ref_path = '/g/data/ia39/ncra/fire/AGCD-05i_BOM_ERA5_historical_hres_BARRA-R2_v1_day_FFDI.zarr'
    gcm_path = '/g/data/ia39/ncra/fire/AGCD-05i_BOM_ACCESS-CM2_ssp370_r4i1p1f1_BARPA-R_v1-r1_day_FFDI.zarr'

    ref_data = xr.open_zarr(ref_path)
    hist_data = xr.open_zarr(gcm_path)

    # Chunk the data as needed
    ref_data = ref_data.chunk(chunks={"lat": chunk_size, "lon": chunk_size, "time": -1})
    hist_data = hist_data.chunk(chunks={"lat": chunk_size, "lon": chunk_size, "time": -1})

    fut_data = None  # Set this to the appropriate future data if available

    qme_run('FFDI', ref_data, hist_data, fut_data)
